# OTU_prediction_ML

Scripts for collating data, pre-processing steps, ML model and model evaluation.

## 1. Input data format
`ML_data_formatting.py` takes two input files per sample:

-  `sample_abundance.txt` A two column tab-delimited file containing NCBI accession number (i.e. `NZ_CP030982.1`) and abundance. Each file must be named in the manner shown here, where 'sample' can be replaced with specific identifier.
-  `sample_centrifugeReport.txt` A standard [centrifuge](https://ccb.jhu.edu/software/centrifuge/) output. Each file must be named in the manner shown here, where 'sample' can be replaced with specific identifier.

These files must be in a directory named with the sample identifier (consistent across both files and directory). 
The script takes two user input values:

1. The path to the directory which contains your sample directories.
2. The path to the directory you would like the processed outputs storing.

The processed outputs contain the following data, and are in comma-separated 'txt' files, named: `sample_data.txt`

The data has the following columns:

- **name:** OTU name; generated by centrifuge. Not used in model as non-numeric.
- **taxID:** 1-7 digit number. Unique OTU identifier, used to match centrifuge hits and actual presence.
- **taxRank:** 'Sprecies', 'genus', 'family' etc...
- **genomeSize:** Size of OTU genome. Only meaningful for OTU classification at 'species' or above.
- **numReads:** Total number of reads mapping to OTU.
- **numUniqueReads:** Unique only reads mapping to OTU.
- **abundance:** Number of reads/total reads.
- **genus:** A grouping variable, included only where it can be determined (i.e. classification at 'genus' level and above).
- **presence:** Binary variable, 0 = not present; 1 = present. Determined from original `abundance.txt` files. Used to train and validate predictions.
- **sim_abundance:** 'Real' abundance of reads in each sample. Used to train and validate mode predictions.

**Note:** Script uses biopython to query NCBI to match taxIDs. You will likely need to give your Entrez details (minimum: email).

## 2. Run the ML model

There are two different models available, both require the data generated in step 1, and data must be split into `training` and `test` directories (specified in the script). The model will use the numeric only variables (i.e. `name`, `genus` and `taxRank` are dropped). Per `test` sample performance metrics are provided, along with `overall` metrics. A `Confusion Matrix` is also produced (printed to screen).

1. `ML_RandomForest_OTU_prediction.py` Runs a [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) model.
2. `ML_LogisticRegression_OTU_prediction.py` employs a basic [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model.

You can run either or both models, and then compare them in later steps.

## Example output

### Overall Performance Metrics:

**Overall Accuracy:** 0.9985252439759422

### Overall Classification Report:

| Class | precision | recall | f1-score | support |
| --- | --- | --- | --- | --- |
| absence | 1.00 | 1.00 | 1.00 | 77728 |
| presence | 0.80 | 0.72 | 0.76 | 251 |
| accuracy |     |     | 1.00 | 77979 |
| macro avg | 0.90 | 0.86 | 0.88 | 77979 |
| weighted avg | 1.00 | 1.00 | 1.00 | 77979 |

### Confusion matrix:
![confusion_matrix](https://github.com/DrATedder/OTU_prediction_ML/blob/1403a9a9f794bc7d514c462da94a13f970aeb7f2/images/confusion_matrix.png)


## 3. Check important features
`ML_features.py` Determines which of the data features (based on the `training` data set) is driving the model. Requires the location of the directory housing the training data set only, and produces a `bar chart`.

## Example output

![Important_features](https://github.com/DrATedder/OTU_prediction_ML/blob/1403a9a9f794bc7d514c462da94a13f970aeb7f2/images/important_factors.png)

## 4. Compare model performance
`compare_ML_models.py` will calculate basic model statistics (also seen in the output of the individual models), along with plotting ROC curves and calculating AUC scores to aid in evaluation. Requires the location of your `test data` directory.

## Example output

![ROC curve with AUC score](https://github.com/DrATedder/OTU_prediction_ML/blob/4fd759f06588a24ecfe3d9bee584ebc7302236dd/images/ROC_AUC.png)


### Note
The scripts were developed in `Jupyter Notebook` and have not been robustly tested in command line python.
